{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2cfb6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL1\\anaconda3\\envs\\myexamenv\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "c:\\Users\\TL1\\anaconda3\\envs\\myexamenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from jinja2 import Template\n",
    "import torch.nn as nn\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# --- HARDCODED CONSTANTS (Llama 3.2 Updates) ---\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B\"\n",
    "TASK_FILE_NAME = \"samanantar_hindi.json\"\n",
    "SHOTS = 5\n",
    "MOD = \"GV_trace_latest_up\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1956724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BASE_DIR =\"new_folder\"\n",
    "\n",
    "DATA_PATH_ROOT = BASE_DIR \n",
    "\n",
    "# --- Directory Setup ---\n",
    "os.makedirs(os.path.join(BASE_DIR, 'data_token'), exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, 'matrix'), exist_ok=True)\n",
    "\n",
    "# Set Device\n",
    "# Checks for CUDA and falls back to CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5534d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utility Functions ---\n",
    "\n",
    "def data_construct(raw_instances, instruction, shot=SHOTS):\n",
    "    \"\"\"Formats raw instances and instruction into a list of messages for the chat template.\"\"\"\n",
    "    if isinstance(instruction, list):\n",
    "        instruction = \"\\n\".join(instruction)\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": instruction.strip()}]\n",
    "    \n",
    "    if shot > 0 and raw_instances:\n",
    "        num_examples = min(shot, len(raw_instances))\n",
    "        few_shot_examples = raw_instances[:num_examples] \n",
    "        \n",
    "        for example in few_shot_examples:\n",
    "            user_content = example[\"input\"].strip()\n",
    "            messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "            \n",
    "            output = example[\"output\"]\n",
    "            assistant_content = output[0].strip() if isinstance(output, list) else output.strip()\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_content})\n",
    "\n",
    "    return messages\n",
    "\n",
    "def find_all_sublists(haystack, needle):\n",
    "    \"\"\"Finds all start indices of a sublist (needle) in a list (haystack).\"\"\"\n",
    "    sublist_indices = []\n",
    "    n = len(needle)\n",
    "    for i in range(len(haystack) - n + 1):\n",
    "        if haystack[i:i + n] == needle:\n",
    "            sublist_indices.append(list(range(i, i + n)))\n",
    "    return sublist_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3795e910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load data from: new_folder\\samanantar_hindi_10.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL1\\anaconda3\\envs\\myexamenv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\TL1\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-3B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# --- Llama 3.2 Marker Token IDs (No Change Needed Here) ---\n",
    "\n",
    "sub_squence = {\n",
    "    \"<|start_header_id|>\":[128006], \n",
    "    \"<|end_header_id|>\":[128007],          \n",
    "    \"<|eot_id|>\":[128009]                  \n",
    "}\n",
    "sub_squence_list = [\n",
    "    sub_squence[\"<|start_header_id|>\"], \n",
    "    sub_squence[\"<|end_header_id|>\"],\n",
    "    sub_squence[\"<|eot_id|>\"]\n",
    "]\n",
    "\n",
    "# --- Hardcoded Translation Task Instruction ---\n",
    "# Since your new JSON file is a raw list of data, the task instruction must be defined here.\n",
    "INSTRUCTION = \"Translate the following English sentence to Hindi. Only output the Hindi translation.\"\n",
    "# Update the TASK_FILE_NAME constant to match the uploaded file\n",
    "TASK_FILE_NAME = \"samanantar_hindi_10.json\"\n",
    "# --- Data Loading and Preparation (ADJUSTED FOR NEW FILE STRUCTURE) ---\n",
    "\n",
    "task = TASK_FILE_NAME.split(\"_\")[0]\n",
    "data_path = os.path.join(DATA_PATH_ROOT, TASK_FILE_NAME)\n",
    "print(f\"Attempting to load data from: {data_path}\")\n",
    "\n",
    "try:\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f: # Added encoding for safety with Hindi\n",
    "        # The JSON load result is the list of instances itself\n",
    "        instance_data = json.load(f)\n",
    "        \n",
    "        # Reformat instances to match old 'input'/'output' structure for data_construct\n",
    "        instance = []\n",
    "        for item in instance_data:\n",
    "            instance.append({\n",
    "                # 'src' is the input for the model, 'tgt' is the label/output\n",
    "                \"input\": item[\"input\"][\"src\"], \n",
    "                \"output\": [item[\"input\"][\"tgt\"]] \n",
    "            })\n",
    "\n",
    "        data_number = len(instance)\n",
    "        \n",
    "        # Split data\n",
    "        train, test = instance[:data_number//2], instance[data_number//2:]\n",
    "        \n",
    "        # Prepare list of messages for each train/test sample\n",
    "        # Note: 'train_list' is a list where each element is the full chat history for one sample.\n",
    "        train_list = [data_construct(train, INSTRUCTION, shot=SHOTS) for _ in train]\n",
    "        test_list = [data_construct(test, INSTRUCTION, shot=SHOTS) for _ in test]\n",
    "        train_list=[train_list[0]]\n",
    "        \n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nüö® ERROR: Task file not found at {data_path}. \\n   Please check your DATA_PATH_ROOT.\")\n",
    "    raise\n",
    "except json.JSONDecodeError:\n",
    "    print(\"\\nüö® ERROR: Could not decode JSON. Ensure the file is correctly formatted.\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c809254b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'system',\n",
       "   'content': 'Translate the following English sentence to Hindi. Only output the Hindi translation.'},\n",
       "  {'role': 'user',\n",
       "   'content': \"However, Paes, who was partnering Australia's Paul Hanley, could only go as far as the quarterfinals where they lost to Bhupathi and Knowles\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': '‡§Ü‡§∏‡•ç‡§ü‡•ç‡§∞‡•á‡§≤‡§ø‡§Ø‡§æ ‡§ï‡•á ‡§™‡§æ‡§≤ ‡§π‡•á‡§®‡§≤‡•Ä ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ú‡•ã‡•ú‡•Ä ‡§¨‡§®‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§™‡•á‡§∏ ‡§Æ‡§ø‡§Ø‡§æ‡§Æ‡•Ä ‡§Æ‡•á‡§Ç ‡§ï‡•ç‡§µ‡§æ‡§∞‡•ç‡§ü‡§∞‡§´‡§æ‡§á‡§®‡§≤ ‡§§‡§ï ‡§π‡•Ä ‡§™‡§π‡•Å‡§Ç‡§ö ‡§∏‡§ï‡•á ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø ‡§á‡§∏ ‡§¶‡•å‡§∞ ‡§Æ‡•á‡§Ç ‡§â‡§®‡•ç‡§π‡•á‡§Ç ‡§≠‡•Ç‡§™‡§§‡§ø ‡§î‡§∞ ‡§®‡•ã‡§≤‡•ç‡§∏ ‡§®‡•á ‡§π‡§∞‡§æ‡§Ø‡§æ ‡§•‡§æ‡•§'},\n",
       "  {'role': 'user',\n",
       "   'content': 'Whosoever desires the reward of the world, with Allah is the reward of the world and of the Everlasting Life. Allah is the Hearer, the Seer.'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '‡§î‡§∞ ‡§ú‡•ã ‡§∂‡§ñ‡•ç‡§∏ (‡§Ö‡§™‡§®‡•á ‡§Ü‡§Æ‡§æ‡§≤ ‡§ï‡§æ) ‡§¨‡§¶‡§≤‡§æ ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‡§π‡•Ä ‡§Æ‡•á‡§Ç ‡§ö‡§æ‡§π‡§§‡§æ ‡§π‡•à ‡§§‡•ã ‡§ñ‡§º‡•Å‡§¶‡§æ ‡§ï‡•á ‡§™‡§æ‡§∏ ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‡§µ ‡§Ü‡§ñ‡§º‡§ø‡§∞‡§§ ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§ï‡§æ ‡§Ö‡§ú‡•ç‡§∞ ‡§Æ‡•å‡§ú‡•Ç‡§¶ ‡§π‡•à ‡§î‡§∞ ‡§ñ‡§º‡•Å‡§¶‡§æ ‡§§‡•ã ‡§π‡§∞ ‡§∂‡§ñ‡•ç‡§∏ ‡§ï‡•Ä ‡§∏‡•Å‡§®‡§§‡§æ ‡§î‡§∞ ‡§∏‡§¨‡§ï‡•ã ‡§¶‡•á‡§ñ‡§§‡§æ ‡§π‡•à'},\n",
       "  {'role': 'user',\n",
       "   'content': 'The value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness.'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '‡§ú‡•à‡§µ-‡§Æ‡§Ç‡§°‡§≤ ‡§Æ‡•á‡§Ç ‡§ï‡•Ä‡§°‡§º‡•ã‡§Ç ‡§ï‡§æ ‡§Æ‡•Ç‡§≤‡•ç‡§Ø ‡§¨‡§π‡•Å‡§§ ‡§π‡•à, ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø ‡§™‡•ç‡§∞‡§ú‡§æ‡§§‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§∏‡§Æ‡•É‡§¶‡•ç‡§ß‡§ø ‡§ï‡•á ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§Æ‡•á‡§Ç ‡§â‡§®‡§ï‡•Ä ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§Ö‡§®‡•ç‡§Ø ‡§ú‡•Ä‡§µ ‡§∏‡§Æ‡•Ç‡§π‡•ã‡§Ç ‡§∏‡•á ‡§ú‡§º‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§π‡•à‡•§'},\n",
       "  {'role': 'user',\n",
       "   'content': 'Mithali To Anchor Indian Team Against Australia in ODIs'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '‡§Ü‡§∏‡•ç‡§ü‡•ç‡§∞‡•á‡§≤‡§ø‡§Ø‡§æ ‡§ï‡•á ‡§ñ‡§ø‡§≤‡§æ‡§´ ‡§µ‡§®‡§°‡•á ‡§ü‡•Ä‡§Æ ‡§ï‡•Ä ‡§ï‡§Æ‡§æ‡§® ‡§Æ‡§ø‡§§‡§æ‡§≤‡•Ä ‡§ï‡•ã'},\n",
       "  {'role': 'user',\n",
       "   'content': 'After the assent of the Honble President on 8thSeptember, 2016, the 101thConstitutional Amendment Act, 2016 came into existence'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '8 ‡§∏‡§ø‡§§‡§Æ‡•ç\\u200d‡§¨‡§∞, 2016 ‡§ï‡•ã ‡§Æ‡§æ‡§®‡§®‡•Ä‡§Ø ‡§∞‡§æ‡§∑‡•ç\\u200d‡§ü‡•ç‡§∞‡§™‡§§‡§ø ‡§ï‡•Ä ‡§∏‡•ç\\u200d‡§µ‡•Ä‡§ï‡•É‡§§‡§ø ‡§Æ‡§ø‡§≤‡§®‡•á ‡§ï‡•á ‡§¨‡§æ‡§¶ 101‡§µ‡§æ‡§Ç ‡§∏‡§Ç‡§µ‡§ø‡§ß‡§æ‡§® ‡§∏‡§Ç‡§∂‡•ã‡§ß‡§® ‡§Ö‡§ß‡§ø‡§®‡§ø‡§Ø‡§Æ, 2016 ‡§Ö‡§∏‡•ç‡§§‡§ø‡§§‡•ç\\u200d‡§µ ‡§Æ‡•á‡§Ç ‡§Ü‡§Ø‡§æ'}]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "666437ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data and calculating trace indices...\n",
      "Using manually defined Llama 3 chat template.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Processing data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 141.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Translate the following English sentence to Hindi. Only output the Hindi translation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "However, Paes, who was partnering Australia's Paul Hanley, could only go as far as the quarterfinals where they lost to Bhupathi and Knowles<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "‡§Ü‡§∏‡•ç‡§ü‡•ç‡§∞‡•á‡§≤‡§ø‡§Ø‡§æ ‡§ï‡•á ‡§™‡§æ‡§≤ ‡§π‡•á‡§®‡§≤‡•Ä ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ú‡•ã‡•ú‡•Ä ‡§¨‡§®‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§™‡•á‡§∏ ‡§Æ‡§ø‡§Ø‡§æ‡§Æ‡•Ä ‡§Æ‡•á‡§Ç ‡§ï‡•ç‡§µ‡§æ‡§∞‡•ç‡§ü‡§∞‡§´‡§æ‡§á‡§®‡§≤ ‡§§‡§ï ‡§π‡•Ä ‡§™‡§π‡•Å‡§Ç‡§ö ‡§∏‡§ï‡•á ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø ‡§á‡§∏ ‡§¶‡•å‡§∞ ‡§Æ‡•á‡§Ç ‡§â‡§®‡•ç‡§π‡•á‡§Ç ‡§≠‡•Ç‡§™‡§§‡§ø ‡§î‡§∞ ‡§®‡•ã‡§≤‡•ç‡§∏ ‡§®‡•á ‡§π‡§∞‡§æ‡§Ø‡§æ ‡§•‡§æ‡•§<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Whosoever desires the reward of the world, with Allah is the reward of the world and of the Everlasting Life. Allah is the Hearer, the Seer.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "‡§î‡§∞ ‡§ú‡•ã ‡§∂‡§ñ‡•ç‡§∏ (‡§Ö‡§™‡§®‡•á ‡§Ü‡§Æ‡§æ‡§≤ ‡§ï‡§æ) ‡§¨‡§¶‡§≤‡§æ ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‡§π‡•Ä ‡§Æ‡•á‡§Ç ‡§ö‡§æ‡§π‡§§‡§æ ‡§π‡•à ‡§§‡•ã ‡§ñ‡§º‡•Å‡§¶‡§æ ‡§ï‡•á ‡§™‡§æ‡§∏ ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‡§µ ‡§Ü‡§ñ‡§º‡§ø‡§∞‡§§ ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§ï‡§æ ‡§Ö‡§ú‡•ç‡§∞ ‡§Æ‡•å‡§ú‡•Ç‡§¶ ‡§π‡•à ‡§î‡§∞ ‡§ñ‡§º‡•Å‡§¶‡§æ ‡§§‡•ã ‡§π‡§∞ ‡§∂‡§ñ‡•ç‡§∏ ‡§ï‡•Ä ‡§∏‡•Å‡§®‡§§‡§æ ‡§î‡§∞ ‡§∏‡§¨‡§ï‡•ã ‡§¶‡•á‡§ñ‡§§‡§æ ‡§π‡•à<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "The value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "‡§ú‡•à‡§µ-‡§Æ‡§Ç‡§°‡§≤ ‡§Æ‡•á‡§Ç ‡§ï‡•Ä‡§°‡§º‡•ã‡§Ç ‡§ï‡§æ ‡§Æ‡•Ç‡§≤‡•ç‡§Ø ‡§¨‡§π‡•Å‡§§ ‡§π‡•à, ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø ‡§™‡•ç‡§∞‡§ú‡§æ‡§§‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§∏‡§Æ‡•É‡§¶‡•ç‡§ß‡§ø ‡§ï‡•á ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§Æ‡•á‡§Ç ‡§â‡§®‡§ï‡•Ä ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§Ö‡§®‡•ç‡§Ø ‡§ú‡•Ä‡§µ ‡§∏‡§Æ‡•Ç‡§π‡•ã‡§Ç ‡§∏‡•á ‡§ú‡§º‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§π‡•à‡•§<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Mithali To Anchor Indian Team Against Australia in ODIs<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "‡§Ü‡§∏‡•ç‡§ü‡•ç‡§∞‡•á‡§≤‡§ø‡§Ø‡§æ ‡§ï‡•á ‡§ñ‡§ø‡§≤‡§æ‡§´ ‡§µ‡§®‡§°‡•á ‡§ü‡•Ä‡§Æ ‡§ï‡•Ä ‡§ï‡§Æ‡§æ‡§® ‡§Æ‡§ø‡§§‡§æ‡§≤‡•Ä ‡§ï‡•ã<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "After the assent of the Honble President on 8thSeptember, 2016, the 101thConstitutional Amendment Act, 2016 came into existence<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "8 ‡§∏‡§ø‡§§‡§Æ‡•ç‚Äç‡§¨‡§∞, 2016 ‡§ï‡•ã ‡§Æ‡§æ‡§®‡§®‡•Ä‡§Ø ‡§∞‡§æ‡§∑‡•ç‚Äç‡§ü‡•ç‡§∞‡§™‡§§‡§ø ‡§ï‡•Ä ‡§∏‡•ç‚Äç‡§µ‡•Ä‡§ï‡•É‡§§‡§ø ‡§Æ‡§ø‡§≤‡§®‡•á ‡§ï‡•á ‡§¨‡§æ‡§¶ 101‡§µ‡§æ‡§Ç ‡§∏‡§Ç‡§µ‡§ø‡§ß‡§æ‡§® ‡§∏‡§Ç‡§∂‡•ã‡§ß‡§® ‡§Ö‡§ß‡§ø‡§®‡§ø‡§Ø‡§Æ, 2016 ‡§Ö‡§∏‡•ç‡§§‡§ø‡§§‡•ç‚Äç‡§µ ‡§Æ‡•á‡§Ç ‡§Ü‡§Ø‡§æ<|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Tokenization and Tracing Indexing (Train Data) ---\n",
    "\n",
    "# Re-defining the template manually in case the tokenizer was loaded without it\n",
    "# This assumes the Llama 3.x template (since we are using Llama 3.2 model)\n",
    "LLAMA3_CHAT_TEMPLATE = (\n",
    "    \"{% for message in messages %}\"\n",
    "        \"{% if loop.index == 1 %}\"\n",
    "            \"{{ '<|begin_of_text|>' }}\"\n",
    "        \"{% endif %}\"\n",
    "        \"{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' }}\"\n",
    "        \"{{ message['content'] | trim }}\"\n",
    "        \"{{ '<|eot_id|>' }}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}\"\n",
    "        \"{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"\n",
    "    \"{% endif %}\"\n",
    ")\n",
    "\n",
    "# NOTE: If you haven't assigned this manually in the loading cell, do it here to prevent the error\n",
    "# tokenizer.default_chat_template = LLAMA3_CHAT_TEMPLATE\n",
    "\n",
    "\n",
    "train_data_token_dir = os.path.join(BASE_DIR, f'data_token/{task}')\n",
    "if \"trace\" not in MOD:\n",
    "    train_file = os.path.join(train_data_token_dir, f'train_{str(SHOTS)}.pkl')\n",
    "else:\n",
    "    train_file = os.path.join(train_data_token_dir, f'train_trace_{str(SHOTS)}.pkl')\n",
    "\n",
    "if os.path.exists(train_file):\n",
    "    print(f\"Loading pre-tokenized train data from {train_file}\")\n",
    "    with open(train_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        train_token = data[\"inputs\"]\n",
    "        indexs = data.get(\"indexs\", [])\n",
    "else:\n",
    "    print(\"Tokenizing train data and calculating trace indices...\")\n",
    "    train_token = []\n",
    "    indexs = []\n",
    "    \n",
    "    # Ensure all required variables (SHOTS, train_list, etc.) are defined \n",
    "    # and the tokenizer template is set BEFORE this cell runs.\n",
    "    \n",
    "    # Check if the attribute exists; if not, use the manually defined one:\n",
    "    try:\n",
    "        template_str = tokenizer.default_chat_template\n",
    "    except AttributeError:\n",
    "        print(\"Using manually defined Llama 3 chat template.\")\n",
    "        template_str = LLAMA3_CHAT_TEMPLATE\n",
    "        \n",
    "    template = Template(template_str)\n",
    "    \n",
    "    progress_bar = tqdm(total=len(train_list), desc='Train Processing data')\n",
    "    \n",
    "    for message in train_list:\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        # Preserve original code's empty BOS/EOS - NOTE: This might lead to suboptimal results\n",
    "        # as Llama 3 expects the tokens, but we honor the original code's structure.\n",
    "        bos_token = \"\" \n",
    "        eos_token = \"\"\n",
    "        \n",
    "        # Use template to render the message\n",
    "        result = template.render(messages=message, bos_token=bos_token, eos_token=eos_token, add_generation_prompt=False).replace(\"<spe>\",\" \")\n",
    "        print(result)\n",
    "        input_ids = tokenizer.encode(result)\n",
    "        train_token.append(input_ids)\n",
    "        \n",
    "\n",
    "        if \"trace\" in MOD:\n",
    "            track_index = []\n",
    "            for seq in sub_squence_list:\n",
    "                 track_index.extend(find_all_sublists(input_ids, seq))\n",
    "            \n",
    "            lat_list = [item for sublist in track_index for item in sublist]\n",
    "            indexs.append(sorted(list(set(lat_list))))\n",
    "            \n",
    "    progress_bar.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4848d27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[128000,\n",
       "  128000,\n",
       "  128006,\n",
       "  9125,\n",
       "  128007,\n",
       "  271,\n",
       "  28573,\n",
       "  279,\n",
       "  2768,\n",
       "  6498,\n",
       "  11914,\n",
       "  311,\n",
       "  45080,\n",
       "  13,\n",
       "  8442,\n",
       "  2612,\n",
       "  279,\n",
       "  45080,\n",
       "  14807,\n",
       "  13,\n",
       "  128009,\n",
       "  128006,\n",
       "  882,\n",
       "  128007,\n",
       "  271,\n",
       "  11458,\n",
       "  11,\n",
       "  16056,\n",
       "  288,\n",
       "  11,\n",
       "  889,\n",
       "  574,\n",
       "  70220,\n",
       "  8494,\n",
       "  596,\n",
       "  7043,\n",
       "  21296,\n",
       "  3258,\n",
       "  11,\n",
       "  1436,\n",
       "  1193,\n",
       "  733,\n",
       "  439,\n",
       "  3117,\n",
       "  439,\n",
       "  279,\n",
       "  8502,\n",
       "  12085,\n",
       "  82,\n",
       "  1405,\n",
       "  814,\n",
       "  5675,\n",
       "  311,\n",
       "  31930,\n",
       "  455,\n",
       "  67631,\n",
       "  323,\n",
       "  14521,\n",
       "  645,\n",
       "  128009,\n",
       "  128006,\n",
       "  78191,\n",
       "  128007,\n",
       "  271,\n",
       "  102393,\n",
       "  79468,\n",
       "  100431,\n",
       "  86133,\n",
       "  101385,\n",
       "  100322,\n",
       "  24810,\n",
       "  48909,\n",
       "  35470,\n",
       "  84736,\n",
       "  100391,\n",
       "  85410,\n",
       "  101276,\n",
       "  92911,\n",
       "  44747,\n",
       "  48909,\n",
       "  35470,\n",
       "  69258,\n",
       "  101029,\n",
       "  100277,\n",
       "  114868,\n",
       "  44747,\n",
       "  101403,\n",
       "  100306,\n",
       "  35470,\n",
       "  100287,\n",
       "  100391,\n",
       "  35470,\n",
       "  84736,\n",
       "  101755,\n",
       "  92317,\n",
       "  100322,\n",
       "  100497,\n",
       "  44747,\n",
       "  92317,\n",
       "  100271,\n",
       "  48909,\n",
       "  100460,\n",
       "  100273,\n",
       "  105461,\n",
       "  100924,\n",
       "  106357,\n",
       "  92911,\n",
       "  102007,\n",
       "  85410,\n",
       "  44747,\n",
       "  101372,\n",
       "  103515,\n",
       "  100411,\n",
       "  101026,\n",
       "  35470,\n",
       "  48909,\n",
       "  100305,\n",
       "  100311,\n",
       "  65804,\n",
       "  39951,\n",
       "  100504,\n",
       "  100291,\n",
       "  103008,\n",
       "  92317,\n",
       "  100271,\n",
       "  100918,\n",
       "  101358,\n",
       "  100271,\n",
       "  100348,\n",
       "  101843,\n",
       "  80338,\n",
       "  39951,\n",
       "  100358,\n",
       "  100282,\n",
       "  101758,\n",
       "  100782,\n",
       "  100282,\n",
       "  35470,\n",
       "  104579,\n",
       "  100537,\n",
       "  24810,\n",
       "  100518,\n",
       "  100781,\n",
       "  128009,\n",
       "  128006,\n",
       "  882,\n",
       "  128007,\n",
       "  271,\n",
       "  1671,\n",
       "  28246,\n",
       "  2099,\n",
       "  32097,\n",
       "  279,\n",
       "  11565,\n",
       "  315,\n",
       "  279,\n",
       "  1917,\n",
       "  11,\n",
       "  449,\n",
       "  28471,\n",
       "  374,\n",
       "  279,\n",
       "  11565,\n",
       "  315,\n",
       "  279,\n",
       "  1917,\n",
       "  323,\n",
       "  315,\n",
       "  279,\n",
       "  18374,\n",
       "  70529,\n",
       "  9601,\n",
       "  13,\n",
       "  28471,\n",
       "  374,\n",
       "  279,\n",
       "  473,\n",
       "  21449,\n",
       "  11,\n",
       "  279,\n",
       "  1369,\n",
       "  261,\n",
       "  13,\n",
       "  128009,\n",
       "  128006,\n",
       "  78191,\n",
       "  128007,\n",
       "  271,\n",
       "  123941,\n",
       "  100277,\n",
       "  55675,\n",
       "  100446,\n",
       "  100979,\n",
       "  100782,\n",
       "  320,\n",
       "  102315,\n",
       "  100726,\n",
       "  35470,\n",
       "  114663,\n",
       "  100391,\n",
       "  48909,\n",
       "  124693,\n",
       "  110113,\n",
       "  24810,\n",
       "  100291,\n",
       "  101391,\n",
       "  100322,\n",
       "  24810,\n",
       "  85410,\n",
       "  44747,\n",
       "  92317,\n",
       "  100271,\n",
       "  100443,\n",
       "  107767,\n",
       "  24810,\n",
       "  85410,\n",
       "  100266,\n",
       "  100329,\n",
       "  55675,\n",
       "  100639,\n",
       "  100395,\n",
       "  73753,\n",
       "  100296,\n",
       "  24810,\n",
       "  48909,\n",
       "  35470,\n",
       "  84736,\n",
       "  100625,\n",
       "  100291,\n",
       "  101391,\n",
       "  100322,\n",
       "  24810,\n",
       "  100287,\n",
       "  125757,\n",
       "  104538,\n",
       "  126254,\n",
       "  100291,\n",
       "  101001,\n",
       "  100311,\n",
       "  48909,\n",
       "  24810,\n",
       "  121519,\n",
       "  86133,\n",
       "  92317,\n",
       "  114197,\n",
       "  108892,\n",
       "  85410,\n",
       "  100266,\n",
       "  100358,\n",
       "  100639,\n",
       "  100395,\n",
       "  73753,\n",
       "  100296,\n",
       "  24810,\n",
       "  100329,\n",
       "  55675,\n",
       "  104579,\n",
       "  100446,\n",
       "  100979,\n",
       "  100782,\n",
       "  48909,\n",
       "  44747,\n",
       "  69258,\n",
       "  101391,\n",
       "  80338,\n",
       "  24810,\n",
       "  100358,\n",
       "  102927,\n",
       "  65804,\n",
       "  55675,\n",
       "  100291,\n",
       "  121197,\n",
       "  24810,\n",
       "  85410,\n",
       "  100266,\n",
       "  128009,\n",
       "  128006,\n",
       "  882,\n",
       "  128007,\n",
       "  271,\n",
       "  791,\n",
       "  907,\n",
       "  315,\n",
       "  41911,\n",
       "  304,\n",
       "  279,\n",
       "  49554,\n",
       "  8182,\n",
       "  374,\n",
       "  23205,\n",
       "  1606,\n",
       "  814,\n",
       "  85692,\n",
       "  682,\n",
       "  1023,\n",
       "  5496,\n",
       "  5315,\n",
       "  304,\n",
       "  6767,\n",
       "  315,\n",
       "  9606,\n",
       "  90030,\n",
       "  13,\n",
       "  128009,\n",
       "  128006,\n",
       "  78191,\n",
       "  128007,\n",
       "  271,\n",
       "  100346,\n",
       "  100266,\n",
       "  100299,\n",
       "  12,\n",
       "  88344,\n",
       "  121260,\n",
       "  92317,\n",
       "  100271,\n",
       "  48909,\n",
       "  104686,\n",
       "  120159,\n",
       "  48909,\n",
       "  24810,\n",
       "  92317,\n",
       "  102959,\n",
       "  100305,\n",
       "  102875,\n",
       "  102223,\n",
       "  85410,\n",
       "  101411,\n",
       "  48909,\n",
       "  100305,\n",
       "  100311,\n",
       "  65804,\n",
       "  39951,\n",
       "  84736,\n",
       "  86133,\n",
       "  100346,\n",
       "  100444,\n",
       "  100322,\n",
       "  100311,\n",
       "  48909,\n",
       "  44747,\n",
       "  100768,\n",
       "  113969,\n",
       "  101782,\n",
       "  39951,\n",
       "  48909,\n",
       "  35470,\n",
       "  92317,\n",
       "  106820,\n",
       "  35470,\n",
       "  92317,\n",
       "  100271,\n",
       "  102364,\n",
       "  44747,\n",
       "  69258,\n",
       "  107923,\n",
       "  100305,\n",
       "  24810,\n",
       "  101248,\n",
       "  100305,\n",
       "  100277,\n",
       "  102646,\n",
       "  100768,\n",
       "  116487,\n",
       "  100311,\n",
       "  69258,\n",
       "  35470,\n",
       "  100277,\n",
       "  118758,\n",
       "  100314,\n",
       "  100612,\n",
       "  24810,\n",
       "  85410,\n",
       "  100436,\n",
       "  128009,\n",
       "  128006,\n",
       "  882,\n",
       "  128007,\n",
       "  271,\n",
       "  44,\n",
       "  411,\n",
       "  8115,\n",
       "  2057,\n",
       "  49872,\n",
       "  7904,\n",
       "  8068,\n",
       "  30241,\n",
       "  8494,\n",
       "  304,\n",
       "  53832,\n",
       "  3957,\n",
       "  128009,\n",
       "  128006,\n",
       "  78191,\n",
       "  128007,\n",
       "  271,\n",
       "  102393,\n",
       "  79468,\n",
       "  100431,\n",
       "  86133,\n",
       "  101385,\n",
       "  100322,\n",
       "  24810,\n",
       "  48909,\n",
       "  35470,\n",
       "  100639,\n",
       "  100482,\n",
       "  103534,\n",
       "  120172,\n",
       "  100361,\n",
       "  35470,\n",
       "  101002,\n",
       "  102232,\n",
       "  48909,\n",
       "  44747,\n",
       "  102359,\n",
       "  100306,\n",
       "  92317,\n",
       "  100428,\n",
       "  100391,\n",
       "  44747,\n",
       "  48909,\n",
       "  55675,\n",
       "  128009,\n",
       "  128006,\n",
       "  882,\n",
       "  128007,\n",
       "  271,\n",
       "  6153,\n",
       "  279,\n",
       "  1089,\n",
       "  306,\n",
       "  315,\n",
       "  279,\n",
       "  16958,\n",
       "  901,\n",
       "  4900,\n",
       "  389,\n",
       "  220,\n",
       "  23,\n",
       "  339,\n",
       "  30649,\n",
       "  11,\n",
       "  220,\n",
       "  679,\n",
       "  21,\n",
       "  11,\n",
       "  279,\n",
       "  220,\n",
       "  4645,\n",
       "  339,\n",
       "  1128,\n",
       "  10663,\n",
       "  278,\n",
       "  22454,\n",
       "  3298,\n",
       "  11,\n",
       "  220,\n",
       "  679,\n",
       "  21,\n",
       "  3782,\n",
       "  1139,\n",
       "  14209,\n",
       "  128009,\n",
       "  128006,\n",
       "  78191,\n",
       "  128007,\n",
       "  271,\n",
       "  23,\n",
       "  69258,\n",
       "  100428,\n",
       "  88344,\n",
       "  103265,\n",
       "  105800,\n",
       "  11,\n",
       "  220,\n",
       "  679,\n",
       "  21,\n",
       "  48909,\n",
       "  55675,\n",
       "  92317,\n",
       "  116369,\n",
       "  101415,\n",
       "  100303,\n",
       "  102580,\n",
       "  103265,\n",
       "  100366,\n",
       "  124303,\n",
       "  39951,\n",
       "  48909,\n",
       "  44747,\n",
       "  69258,\n",
       "  103265,\n",
       "  100299,\n",
       "  101795,\n",
       "  102861,\n",
       "  39951,\n",
       "  92317,\n",
       "  117189,\n",
       "  35470,\n",
       "  48909,\n",
       "  35470,\n",
       "  100276,\n",
       "  100612,\n",
       "  220,\n",
       "  4645,\n",
       "  100299,\n",
       "  100515,\n",
       "  69258,\n",
       "  114298,\n",
       "  103245,\n",
       "  100306,\n",
       "  69258,\n",
       "  113757,\n",
       "  105951,\n",
       "  61196,\n",
       "  101801,\n",
       "  100556,\n",
       "  107300,\n",
       "  11,\n",
       "  220,\n",
       "  679,\n",
       "  21,\n",
       "  105739,\n",
       "  100365,\n",
       "  100428,\n",
       "  103265,\n",
       "  100299,\n",
       "  92317,\n",
       "  100271,\n",
       "  103813,\n",
       "  24810,\n",
       "  128009]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5880647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new list [128001, 128001, 128001, ...]\n",
    "new_list = [128001 for x in train_token[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abd24706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2,\n",
       "  4,\n",
       "  20,\n",
       "  21,\n",
       "  23,\n",
       "  59,\n",
       "  60,\n",
       "  62,\n",
       "  143,\n",
       "  144,\n",
       "  146,\n",
       "  183,\n",
       "  184,\n",
       "  186,\n",
       "  275,\n",
       "  276,\n",
       "  278,\n",
       "  303,\n",
       "  304,\n",
       "  306,\n",
       "  374,\n",
       "  375,\n",
       "  377,\n",
       "  391,\n",
       "  392,\n",
       "  394,\n",
       "  423,\n",
       "  424,\n",
       "  426,\n",
       "  463,\n",
       "  464,\n",
       "  466,\n",
       "  532]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b043814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-tokenized test data from new_folder\\data_token/samanantar\\test_5.pkl\n"
     ]
    }
   ],
   "source": [
    "# --- Tokenization and Label Extraction (Test Data) ---\n",
    "\n",
    "test_data_token_dir = os.path.join(BASE_DIR, f'data_token/{task}')\n",
    "test_file = os.path.join(test_data_token_dir, f'test_{str(SHOTS)}.pkl')\n",
    "\n",
    "# Get task instruction template (defined in the data loading cell)\n",
    "# This uses the manually defined template if the tokenizer doesn't have it set.\n",
    "try:\n",
    "    template_str = tokenizer.default_chat_template\n",
    "except AttributeError:\n",
    "    # Assuming LLAMA3_CHAT_TEMPLATE was defined earlier\n",
    "    template_str = LLAMA3_CHAT_TEMPLATE \n",
    "\n",
    "template = Template(template_str)\n",
    "\n",
    "if os.path.exists(test_file):\n",
    "    print(f\"Loading pre-tokenized test data from {test_file}\")\n",
    "    with open(test_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        test_token = data[\"inputs\"]\n",
    "        labels = data[\"labels\"]\n",
    "else:\n",
    "    print(\"Tokenizing test data and extracting labels...\")\n",
    "    test_token = []\n",
    "    labels = []\n",
    "    \n",
    "    progress_bar = tqdm(total=len(test_list), desc='Test Processing data') \n",
    "\n",
    "    for message_full in test_list:\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        prompt, output = message_full[:-1], message_full[-1]\n",
    "        \n",
    "        bos_token = \"\"\n",
    "        eos_token = \"\"\n",
    "        \n",
    "        # Use template to render the prompt\n",
    "        result = template.render(messages=prompt, bos_token=bos_token, eos_token=eos_token, add_generation_prompt=True) # NOTE: added add_generation_prompt=True here, which is standard when preparing the prompt for generation.\n",
    "        test_token.append(tokenizer.encode(result))\n",
    "        labels.append(output[\"content\"])\n",
    "        \n",
    "    progress_bar.close()\n",
    "    \n",
    "    # --- FIX: Ensure the directory exists before writing ---\n",
    "    os.makedirs(test_data_token_dir, exist_ok=True)\n",
    "    \n",
    "    with open(test_file, 'wb') as f:\n",
    "        pickle.dump({\"inputs\": test_token, \"labels\": labels}, f)\n",
    "    print(f\"Saved tokenized test data to {test_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e26c190e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model for gradient tracing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [01:51<00:00, 55.92s/it] \n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:05<00:00,  2.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient tracing process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 samples due to length > 1300.\n",
      "Gradient tracing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Model Loading and Gradient Tracing ---\n",
    "\n",
    "print(\"Loading model for gradient tracing...\")\n",
    "\n",
    "# Load the model with bfloat16 and 'auto' device map\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    token=True\n",
    ")\n",
    "model.train()\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "# Llama 3.2 3B FFN intermediate size is 11008 (32 layers)\n",
    "out_data = [[0.0] * 8192 for _ in range(28)] \n",
    "ss = 0\n",
    "\n",
    "print(\"Starting gradient tracing process...\")\n",
    "progress_bar = tqdm(total=len(train_token), desc='Getting data')\n",
    "\n",
    "for input_ids, index in zip(train_token, indexs):\n",
    "    progress_bar.update(1)\n",
    "    \n",
    "    if len(input_ids) > 1300:\n",
    "        ss += 1\n",
    "        continue\n",
    "    \n",
    "    input_index = [i - 1 for i in index] \n",
    "    label_token = [input_ids[i] for i in index]\n",
    "\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    label_token = torch.tensor(label_token, dtype=torch.long).to(device)\n",
    "    \n",
    "    output = model(input_ids)\n",
    "    \n",
    "    # Slicing the loss calculation (28 tokens threshold from original code)\n",
    "    slice1 = 28\n",
    "    \n",
    "    logits_slice_1 = output.logits[0, input_index[:slice1], :]\n",
    "    labels_slice_1 = label_token[:slice1]\n",
    "    \n",
    "    logits_slice_2 = output.logits[0, input_index[slice1:], :]\n",
    "    labels_slice_2 = label_token[slice1:]\n",
    "    \n",
    "    loss1 = criterion(logits_slice_1, labels_slice_1)\n",
    "    loss2 = criterion(logits_slice_2, labels_slice_2)\n",
    "    \n",
    "    loss = loss1.mean() + loss2.mean()\n",
    "    \n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Accumulate Gradients for the up_proj layer\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None and \"up_proj\" in name:\n",
    "            layer = int(name.split(\".\")[2])\n",
    "            \n",
    "            grad = torch.sum(param.grad.abs(), dim=1).cpu().tolist()\n",
    "            \n",
    "            out_data[layer] = [a + b for a, b in zip(grad, out_data[layer])]\n",
    "\n",
    "\n",
    "progress_bar.close()\n",
    "print(f\"Skipped {ss} samples due to length > 1300.\")\n",
    "print(\"Gradient tracing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6586305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved tracing matrix to: new_folder\\matrix/samanantar\\GV_trace_latest_up.json\n"
     ]
    }
   ],
   "source": [
    "# --- Save Results ---\n",
    "\n",
    "matrix_dir = os.path.join(BASE_DIR, f'matrix/{task}')\n",
    "os.makedirs(matrix_dir, exist_ok=True)\n",
    "output_path = os.path.join(matrix_dir, f'{MOD}.json')\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(out_data, f)\n",
    "    \n",
    "print(f\"Successfully saved tracing matrix to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myexamenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
